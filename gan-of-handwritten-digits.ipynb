{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!mkdir images/","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:25.627593Z","iopub.execute_input":"2023-07-07T00:07:25.628074Z","iopub.status.idle":"2023-07-07T00:07:26.700759Z","shell.execute_reply.started":"2023-07-07T00:07:25.628039Z","shell.execute_reply":"2023-07-07T00:07:26.699579Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘images/’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, LeakyReLU, BatchNormalization\nfrom tensorflow.keras.models import Sequential, Model\n# from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.legacy import Adam\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.704625Z","iopub.execute_input":"2023-07-07T00:07:26.706852Z","iopub.status.idle":"2023-07-07T00:07:26.714041Z","shell.execute_reply.started":"2023-07-07T00:07:26.706810Z","shell.execute_reply":"2023-07-07T00:07:26.713103Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"IMG_ROWS = 28\nIMG_COLS = 28\nCHANNELS = 1\nIMAGE_SHAPE = (IMG_ROWS, IMG_COLS, CHANNELS)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.715498Z","iopub.execute_input":"2023-07-07T00:07:26.715822Z","iopub.status.idle":"2023-07-07T00:07:26.724690Z","shell.execute_reply.started":"2023-07-07T00:07:26.715794Z","shell.execute_reply":"2023-07-07T00:07:26.723680Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def build_discriminator():\n    \n    model = Sequential()\n    \n    model.add(Flatten(input_shape=IMAGE_SHAPE))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.summary()\n    \n    img = Input(shape=IMAGE_SHAPE)\n    validity = model(img)\n    \n    return Model(img, validity)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.726456Z","iopub.execute_input":"2023-07-07T00:07:26.726793Z","iopub.status.idle":"2023-07-07T00:07:26.736327Z","shell.execute_reply.started":"2023-07-07T00:07:26.726765Z","shell.execute_reply":"2023-07-07T00:07:26.735307Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def build_generator():\n    NOISE_SHAPE = (100, )\n    \n    model = Sequential()\n    \n    model.add(Dense(256, input_shape=NOISE_SHAPE))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(np.prod(IMAGE_SHAPE), activation='tanh'))\n    model.add(Reshape(IMAGE_SHAPE))\n    \n    model.summary()\n    \n    noise = Input(shape=NOISE_SHAPE)\n    img = model(noise)\n    \n    return Model(noise, img)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.739930Z","iopub.execute_input":"2023-07-07T00:07:26.740875Z","iopub.status.idle":"2023-07-07T00:07:26.749554Z","shell.execute_reply.started":"2023-07-07T00:07:26.740845Z","shell.execute_reply":"2023-07-07T00:07:26.748500Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(epochs, batch_size=128, save_interval=50):\n    \n    (X_train, _), (_, _) = mnist.load_data()\n    \n    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n    \n    X_train = np.expand_dims(X_train, axis=3)\n    \n    half_batch = int(batch_size / 2)\n    \n    for epoch in range(epochs):\n        idx = np.random.randint(0, X_train.shape[0], half_batch)\n        imgs = X_train[idx]\n        \n        noise = np.random.normal(0, 1, (half_batch, 100)) # fake data\n        \n        gen_imgs = generator.predict(noise, verbose=0)\n        \n        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n        \n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n        \n        noise = np.random.normal(0, 1, (batch_size, 100))\n        \n        valid_y = np.array([1]* batch_size)\n        \n        g_loss = combined.train_on_batch(noise, valid_y)\n        \n        if epoch % 100 == 0:\n            print(f\"\\nEpoch {epoch}: [D Loss: {d_loss[0]}] [G Loss: {g_loss}]\")\n        \n        if epoch % save_interval == 0:\n            save_imgs(epoch)","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.751043Z","iopub.execute_input":"2023-07-07T00:07:26.751402Z","iopub.status.idle":"2023-07-07T00:07:26.764151Z","shell.execute_reply.started":"2023-07-07T00:07:26.751372Z","shell.execute_reply":"2023-07-07T00:07:26.763165Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def save_imgs(epoch):\n    r, c = 5, 5\n    noise = np.random.normal(0, 1, (r * c, 100))\n    gen_imgs = generator.predict(noise, verbose=0)\n    \n    gen_imgs = 0.5 * gen_imgs + 0.5\n    \n    fig, axs = plt.subplots(r, c)\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n            axs[i, j].axis('off')\n            cnt += 1\n    fig.savefig(f\"/kaggle/working/images/mnist_{epoch}.png\")\n    plt.close()","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.765713Z","iopub.execute_input":"2023-07-07T00:07:26.766136Z","iopub.status.idle":"2023-07-07T00:07:26.774655Z","shell.execute_reply.started":"2023-07-07T00:07:26.766108Z","shell.execute_reply":"2023-07-07T00:07:26.773579Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(0.0002, 0.5)\n\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\ngenerator = build_generator()\ngenerator.compile(loss='binary_crossentropy', optimizer=optimizer)\n\nz = Input(shape=(100,))\nimg = generator(z)\n\ndiscriminator.trainable = False\n\nvalid = discriminator(img)\n\ncombined = Model(z, valid)\ncombined.compile(loss='binary_crossentropy', optimizer=optimizer)\n\ntrain(epochs=6000, batch_size=32, save_interval=200)\n\ngenerator.save('generator_model_test.h5')","metadata":{"execution":{"iopub.status.busy":"2023-07-07T00:07:26.776258Z","iopub.execute_input":"2023-07-07T00:07:26.776704Z","iopub.status.idle":"2023-07-07T00:16:21.076874Z","shell.execute_reply.started":"2023-07-07T00:07:26.776672Z","shell.execute_reply":"2023-07-07T00:16:21.075838Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n flatten_2 (Flatten)         (None, 784)               0         \n                                                                 \n dense_14 (Dense)            (None, 512)               401920    \n                                                                 \n leaky_re_lu_10 (LeakyReLU)  (None, 512)               0         \n                                                                 \n dense_15 (Dense)            (None, 256)               131328    \n                                                                 \n leaky_re_lu_11 (LeakyReLU)  (None, 256)               0         \n                                                                 \n dense_16 (Dense)            (None, 1)                 257       \n                                                                 \n=================================================================\nTotal params: 533,505\nTrainable params: 533,505\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_17 (Dense)            (None, 256)               25856     \n                                                                 \n leaky_re_lu_12 (LeakyReLU)  (None, 256)               0         \n                                                                 \n batch_normalization_6 (Batc  (None, 256)              1024      \n hNormalization)                                                 \n                                                                 \n dense_18 (Dense)            (None, 512)               131584    \n                                                                 \n leaky_re_lu_13 (LeakyReLU)  (None, 512)               0         \n                                                                 \n batch_normalization_7 (Batc  (None, 512)              2048      \n hNormalization)                                                 \n                                                                 \n dense_19 (Dense)            (None, 1024)              525312    \n                                                                 \n leaky_re_lu_14 (LeakyReLU)  (None, 1024)              0         \n                                                                 \n batch_normalization_8 (Batc  (None, 1024)             4096      \n hNormalization)                                                 \n                                                                 \n dense_20 (Dense)            (None, 784)               803600    \n                                                                 \n reshape_2 (Reshape)         (None, 28, 28, 1)         0         \n                                                                 \n=================================================================\nTotal params: 1,493,520\nTrainable params: 1,489,936\nNon-trainable params: 3,584\n_________________________________________________________________\n\nEpoch 0: [D Loss: 0.5288519263267517] [G Loss: 0.7460513114929199]\n\nEpoch 100: [D Loss: 0.020335027016699314] [G Loss: 4.656238555908203]\n\nEpoch 200: [D Loss: 0.09975894168019295] [G Loss: 3.4964025020599365]\n\nEpoch 300: [D Loss: 0.784524142742157] [G Loss: 0.6368386149406433]\n\nEpoch 400: [D Loss: 0.659991979598999] [G Loss: 0.6510007381439209]\n\nEpoch 500: [D Loss: 0.6157329678535461] [G Loss: 0.699596643447876]\n\nEpoch 600: [D Loss: 0.6349517405033112] [G Loss: 0.7867474555969238]\n\nEpoch 700: [D Loss: 0.659775972366333] [G Loss: 0.7481167316436768]\n\nEpoch 800: [D Loss: 0.6529655456542969] [G Loss: 0.7660805583000183]\n\nEpoch 900: [D Loss: 0.6276408433914185] [G Loss: 0.7879798412322998]\n\nEpoch 1000: [D Loss: 0.6300410628318787] [G Loss: 0.7931948900222778]\n\nEpoch 1100: [D Loss: 0.608789324760437] [G Loss: 0.8020957708358765]\n\nEpoch 1200: [D Loss: 0.6035484969615936] [G Loss: 0.8655709028244019]\n\nEpoch 1300: [D Loss: 0.6336014866828918] [G Loss: 0.855532169342041]\n\nEpoch 1400: [D Loss: 0.5362634360790253] [G Loss: 0.8954193592071533]\n\nEpoch 1500: [D Loss: 0.5894509255886078] [G Loss: 0.8813406229019165]\n\nEpoch 1600: [D Loss: 0.7324342429637909] [G Loss: 0.8812081813812256]\n\nEpoch 1700: [D Loss: 0.713708907365799] [G Loss: 0.8072174787521362]\n\nEpoch 1800: [D Loss: 0.5677673816680908] [G Loss: 0.8865331411361694]\n\nEpoch 1900: [D Loss: 0.6269471645355225] [G Loss: 0.8687668442726135]\n\nEpoch 2000: [D Loss: 0.6548907160758972] [G Loss: 0.8569445610046387]\n\nEpoch 2100: [D Loss: 0.5742906332015991] [G Loss: 0.8909715414047241]\n\nEpoch 2200: [D Loss: 0.6537940204143524] [G Loss: 0.899922251701355]\n\nEpoch 2300: [D Loss: 0.6269595921039581] [G Loss: 0.8597291707992554]\n\nEpoch 2400: [D Loss: 0.6033105254173279] [G Loss: 0.9619326591491699]\n\nEpoch 2500: [D Loss: 0.6336910426616669] [G Loss: 0.9336808919906616]\n\nEpoch 2600: [D Loss: 0.6515722274780273] [G Loss: 0.9334136247634888]\n\nEpoch 2700: [D Loss: 0.5598199963569641] [G Loss: 0.9419276714324951]\n\nEpoch 2800: [D Loss: 0.6551096737384796] [G Loss: 0.9393253922462463]\n\nEpoch 2900: [D Loss: 0.6099151968955994] [G Loss: 0.9009329080581665]\n\nEpoch 3000: [D Loss: 0.5686346590518951] [G Loss: 0.8591070771217346]\n\nEpoch 3100: [D Loss: 0.5876014828681946] [G Loss: 0.9943827390670776]\n\nEpoch 3200: [D Loss: 0.6210949420928955] [G Loss: 0.9067602157592773]\n\nEpoch 3300: [D Loss: 0.6708490252494812] [G Loss: 0.9690481424331665]\n\nEpoch 3400: [D Loss: 0.7466227114200592] [G Loss: 0.9425773024559021]\n\nEpoch 3500: [D Loss: 0.6070901453495026] [G Loss: 0.9684016704559326]\n\nEpoch 3600: [D Loss: 0.6029201447963715] [G Loss: 0.8724348545074463]\n\nEpoch 3700: [D Loss: 0.5763503909111023] [G Loss: 1.056977391242981]\n\nEpoch 3800: [D Loss: 0.6312346756458282] [G Loss: 0.9523277282714844]\n\nEpoch 3900: [D Loss: 0.6065185368061066] [G Loss: 0.9435098767280579]\n\nEpoch 4000: [D Loss: 0.5530305802822113] [G Loss: 0.952665388584137]\n\nEpoch 4100: [D Loss: 0.648934930562973] [G Loss: 0.9383951425552368]\n\nEpoch 4200: [D Loss: 0.595893919467926] [G Loss: 1.0737318992614746]\n\nEpoch 4300: [D Loss: 0.644393652677536] [G Loss: 0.934722900390625]\n\nEpoch 4400: [D Loss: 0.6805624067783356] [G Loss: 0.9336695671081543]\n\nEpoch 4500: [D Loss: 0.6481053829193115] [G Loss: 0.9188140630722046]\n\nEpoch 4600: [D Loss: 0.612242579460144] [G Loss: 0.9754166603088379]\n\nEpoch 4700: [D Loss: 0.562924399971962] [G Loss: 0.9705637097358704]\n\nEpoch 4800: [D Loss: 0.6269632577896118] [G Loss: 0.9338573217391968]\n\nEpoch 4900: [D Loss: 0.6088557839393616] [G Loss: 0.8823100328445435]\n\nEpoch 5000: [D Loss: 0.6555313467979431] [G Loss: 0.8451669216156006]\n\nEpoch 5100: [D Loss: 0.6455232501029968] [G Loss: 0.8545007705688477]\n\nEpoch 5200: [D Loss: 0.6389559805393219] [G Loss: 0.9012511968612671]\n\nEpoch 5300: [D Loss: 0.7169067561626434] [G Loss: 0.9056997299194336]\n\nEpoch 5400: [D Loss: 0.648951917886734] [G Loss: 1.0244311094284058]\n\nEpoch 5500: [D Loss: 0.7005550861358643] [G Loss: 0.921082615852356]\n\nEpoch 5600: [D Loss: 0.6347370147705078] [G Loss: 0.8924553394317627]\n\nEpoch 5700: [D Loss: 0.7086673080921173] [G Loss: 0.9139785766601562]\n\nEpoch 5800: [D Loss: 0.8338024616241455] [G Loss: 0.905049204826355]\n\nEpoch 5900: [D Loss: 0.6294016540050507] [G Loss: 0.8465813994407654]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}